{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "military-stewart",
   "metadata": {},
   "source": [
    "Atreish Ramlakhan<br> Aishwarya Singh<br> Nosson Weissman<br>\n",
    "April 13, 2021 \n",
    "AIM 5001, Prof. James Topor  <br>\n",
    "\n",
    "# AIM 5001 Module 12 \n",
    "## Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-minority",
   "metadata": {},
   "source": [
    "As we’ve learned, many organizations rely on sentiment analysis algorithms to help them gauge the opinions \n",
    "of both existing and potential customers. For example, companies such as Amazon, TripAdvisor, Booking.com, \n",
    "WalMart, and Yelp (amongst others) apply sentiment analysis algorithms to the online product/service \n",
    "reviews provided by their customers to better understand how the public perceives competing products and \n",
    "services.\n",
    "Your task for the Module 12 Assignment is to prepare a collection of text documents for use within a \n",
    "sentiment analysis algorithm. The data set you will be working with is sourced from this site: \n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/. Specifically, you will be working with the polarity dataset \n",
    "v2.0, which is comprised of 1000 positive and 1000 negative movie reviews. Each movie review is in the form of free\u0002form text captured from web site postings. To complete this assignment you will need to make use of a fair \n",
    "amount of pre-processing techniques to prepare the content of the reviews for use within a classification \n",
    "model (e.g., strip out punctuation, stop words, “tokenize” the data, etc.).\n",
    "Get started on the Assignment as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "leading-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-vocabulary",
   "metadata": {},
   "source": [
    "1) Download the review_polarity.tar.gz file to your local environment and decompress its contents. The \n",
    "compressed file contains two directories: neg which contains 1000 negative movie reviews; and pos\n",
    "which contains 1000 positive movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "illegal-fraud",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'https://raw.githubusercontent.com/codepharmer/AIM-5001/main/Module_12/review_polarity.tar.gz'\n",
    "thetarfile = path\n",
    "ftpstream = urllib.request.urlopen(thetarfile)\n",
    "thetarfile = tarfile.open(fileobj=ftpstream, mode='r|gz')\n",
    "thetarfile.extractall('.')\n",
    "gz_dir = set([line.split('/')[0] for line in thetarfile.getnames()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pressing-boards",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_textfiles_from_dir(path):\n",
    "    filenames = next(os.walk(path))[2]\n",
    "    files = []\n",
    "    for name in filenames:\n",
    "        files.append(open((path+'/'+name), 'r').read())\n",
    "    return files\n",
    "\n",
    "pos =  get_textfiles_from_dir('./txt_sentoken/pos')\n",
    "neg = get_textfiles_from_dir('./txt_sentoken/neg')\n",
    "# review_file = open('txt_sentoken')\n",
    "# [sent.lower() for sent in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acute-blind",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \" jaws \" is a rare film that grabs your attention before it shows you a single image on screen . \\nthe movie opens with blackness , and only distant , alien-like underwater sounds . \\nthen it comes , the first ominous bars of composer john williams\\' now infamous score . \\ndah-dum . \\nfrom there , director steven spielberg wastes no time , taking us into the water on a midnight swim with a beautiful girl that turns deadly . \\nright away he lets us know how vulnerable we all are floating in the ocean , and once \" jaws \" has attacked , it never relinquishes its grip . \\nperhaps what is most outstanding about \" jaws \" is how spielberg builds the movie . \\nhe works it like a theatrical production , with a first act and a second act . \\nunlike so many modern filmmakers , he has a great deal of restraint , and refuses to show us the shark until the middle of the second act . \\nuntil then , he merely suggests its presence with creepy , subjective underwater shots and williams\\' music . \\nhe\\'s building the tension bit by bit , so when it comes time for the climax , the shark\\'s arrival is truly terrifying . \\nhe doesn\\'t let us get bored with the imagery . \\nthe first act opens with police chief martin brody ( roy scheider ) , a new york cop who has taken an easy , peaceful job running the police station on amity island , a fictitious new england resort town where there hasn\\'t been a murder or a gun fired in 25 years . \\nthe island is shaken up by several vicious great white shark attacks right before the fourth of july , and the mayor , larry vaughn ( murray hamilton ) , doesn\\'t want to shut down the beaches because the island is reliant on summer tourist money . \\nbrody is joined by matt hooper ( richard dreyfuss ) , a young , ambitious shark expert from the marine institute . \\nhooper is as fascinated by the shark as he is determined to help brody stop it -- his knowledge about the exact workings of the shark ( \" it\\'s a perfect engine , an eating machine \" ) make it that much more terrifying . \\nwhen vaughn finally relents , hooper and brody join a crusty old shark killer named quint ( robert shaw ) on his decrepit boat , the orca , to search for the shark . \\nthe entire second act takes place on the orca as the three men hunt the shark , and inevitably , are hunted by it . \\n \" jaws \" is a thriller with a keen sense of humor and an incredible sense of pacing , tension , and horror . \\nit is like ten movies all rolled into one , and it\\'s no wonder it took america by storm in the summer of 1975 , taking in enough money to crown it the box office champ of all time ( until it was unceremoniously dethroned in 1977 by \" star wars \" ) . \\neven today , fascination with this film is on par with hitchcock\\'s \" psycho , \" and it never seems to age . \\nalthough grand new technology exists that makes the technical sequences , including several mechanical sharks , obsolete , none of it could improve the film because it only would lead to overkill . \\nthe technical limitations faced by spielberg in 1975 may have actually produced a better film because it forced him to rely on traditional cinematic elements like pacing , characterization , sharp editing , and creative photography , instead of simply dousing the audience with digital shark effects . \\nscheider , dreyfuss , and shaw were known actors at the time \" jaws \" was made , but none of them had the draw of a robert redford or paul newman . \\nnevertheless , this film guaranteed them all successful careers because each gave an outstanding performance and refused to be overshadowed by the shark . \\nscheider hits just the right notes as a sympathetic husband and father caught in the political quagmire of doing what\\'s right and going against the entire town . \\n \" it\\'s your first summer here , you know , \" mayor vaughn warns him . \\ndreyfuss , who had previously been seen in \" american graffiti \" ( 1973 ) and \" the apprenticeship of duddy kravitz \" ( 1974 ) gives a surprisingly mature , complex performance for someone who had literally only played kids and teenagers . \\nhowever , most outstanding is the gnarled performance by robert shaw as the movie\\'s captain ahab , a performance sorely overlooked by the academy awards . \\nbordering of parody , shaw plays quint as a grizzled old loner whose machismo borders on masochism . \\nhe\\'s slightly deranged , and shaw\\'s performance is almost a caricature . \\nhowever , there is one scene late in the film , when he and brody and hooper are below deck on the orca comparing scars . \\nquint is drawn into telling the story of his experiences aboard the u . s . s . \\nindianapolis , a navy ship in world war ii that was sunk by the japanese . \\nhis tale of floating in the water for more than a week with over 1 , 000 other men while swarms of sharks slowly devoured them them is actually more hair-raising than anything spielberg put on screen . \\nshaw delivers the story in one long take , and it is the best acting in the film . \\nof course , we can\\'t leave out the shark itself ; with its black eyes , endless rows of teeth , and insatiable urge to eat , it is basically the epitome of all mankind\\'s fears about what is unknown and threatening in nature . \\na shark is such a perfect nemesis it is real -- having survived sinch the dinosaurs , great whites do exist , they can be as large as the shark in \" jaws , \" and they are a threat . \\nevery one of spielberg\\'s subjective underwater shots makes us feel queasy because lets us see how we look to the shark : a bunch of writihing , dangling , completely unprotected legs just ready to be chomped into . \\nthe shark in \" jaws \" was actually a combination of actual footage and five different mechanical sharks ( all nicknamed \" bruce \" by the crew ) built to be shot from different angles . \\nmany have forgotten , but \" jaws \" was a sort of precursor to \" waterworld \" ( 1995 ) , a movie\\'s who soggy production and cost overruns had universal studios worried about a bomb . \\nbut , as we can see now , spielberg overcame all the obstacles , and delivered one of the finest primal scare-thrillers ever to come out of hollywood . \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "present-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = CategorizedPlaintextCorpusReader(r'./txt_sentoken', r'.*\\.txt', cat_pattern=r'(\\w+)/*')\n",
    "# print(movie_reviews.categories())\n",
    "# print(movie_reviews.fileids())\n",
    "\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in ['pos', 'neg']\n",
    "             for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "preceding-benchmark",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words(movie_reviews.fileids('pos')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-combining",
   "metadata": {},
   "source": [
    "2) Load the neg and pos directories to your AIM 5001 Github Repository. You need to keep the content of \n",
    "the directories separated since the directories themselves serve as the labels for the classification of \n",
    "the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "resistant-alliance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "en_stops = stopwords.words('english')\n",
    "print ('not' not in en_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-check",
   "metadata": {},
   "source": [
    "**Some stop words in `en_stops` have the potential to change sentiment. An example of this is the word 'not'. For now we will not worry about this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "selected-weather",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25812"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.porter.PorterStemmer()\n",
    "word_set = set()\n",
    "review_dicts = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def process_string_list(string_list):\n",
    "    for i in range(len(string_list)):\n",
    "        word_freqs = {}\n",
    "        review = string_list[i]\n",
    "        for word in tokenizer.tokenize(review):\n",
    "            text = stemmer.stem(word)\n",
    "            if text not in string.punctuation and not text.isnumeric():\n",
    "                if text not in en_stops:\n",
    "                    word_freqs[text] = word_freqs.get(text, 0) + 1\n",
    "                word_set.add(text)\n",
    "        review_dicts.append(word_freqs)\n",
    "    \n",
    "process_string_list(pos)\n",
    "process_string_list(neg)\n",
    "len(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-theater",
   "metadata": {},
   "source": [
    "**NOTE: Reviews are already down_cased as per the README. As such, we did not need to use that code**\n",
    "\n",
    "___Within the folder \"txt_sentoken\" are the 2000 processed down-cased\n",
    "  text files used in Pang/Lee ACL 2004; the names of the two\n",
    "  subdirectories in that folder, \"pos\" and \"neg\", indicate the true\n",
    "  classification (sentiment) of the component files according to our\n",
    "  automatic rating classifier (see section \"Rating Decision\" below).___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "north-falls",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "accomplished-meditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['films', 'adapted', 'from', 'comic', 'books', 'have', ...], ['every', 'now', 'and', 'then', 'a', 'movie', 'comes', ...], ['you', \"'\", 've', 'got', 'mail', 'works', 'alot', ...]]\n"
     ]
    }
   ],
   "source": [
    "pos_reviews = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = movie_reviews.words(fileid)\n",
    "    pos_reviews.append(words)\n",
    "    \n",
    "# print first 3 positive reviews from the pos_reviews list\n",
    "print (pos_reviews[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "union-infrastructure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['films', 'adapted', 'from', 'comic', 'books', 'have', ...],\n",
       " ['every', 'now', 'and', 'then', 'a', 'movie', 'comes', ...],\n",
       " ['you', \"'\", 've', 'got', 'mail', 'works', 'alot', ...],\n",
       " ['\"', 'jaws', '\"', 'is', 'a', 'rare', 'film', 'that', ...],\n",
       " ['moviemaking', 'is', 'a', 'lot', 'like', 'being', ...],\n",
       " ['on', 'june', '30', ',', '1960', ',', 'a', 'self', ...],\n",
       " ['apparently', ',', 'director', 'tony', 'kaye', 'had', ...],\n",
       " ['one', 'of', 'my', 'colleagues', 'was', 'surprised', ...],\n",
       " ['after', 'bloody', 'clashes', 'and', 'independence', ...],\n",
       " ['the', 'american', 'action', 'film', 'has', 'been', ...]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_reviews[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-cleaners",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-honolulu",
   "metadata": {},
   "source": [
    "tokenize\n",
    "text normalization\n",
    "remove syntax\n",
    "leave punctuation\n",
    "### removing emojis?\n",
    "removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-steal",
   "metadata": {},
   "source": [
    "3) Then, using a Jupyter Notebook, construct an algorithm (DO NOT USE scikit-learn COUNTVECTORIZER) \n",
    "that will read the content of each individual movie review from your new Github directories and \n",
    "convert that content into a properly labeled (i.e., POS / NEG or some appropriate proxy thereof) entry \n",
    "within a Pandas dataframe that encompasses all of the possible words contained within the 2000 \n",
    "movie reviews. When finished, the contents of your Pandas dataframe will constitute a term-document \n",
    "matrix for the movie review data. While constructing this term-document matrix within your Pandas \n",
    "dataframe, you should ensure that you remove any punctuation or stop words from the reviews. How \n",
    "you choose to manage the construction and proper labeling of the term-document matrix is up to you \n",
    "as the text mining / Python practitioner to decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "original-afghanistan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_set\n",
    "df_header = list(word_set)\n",
    "df_header.sort()\n",
    "# review_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "informed-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for review in review_dicts:\n",
    "    new_row = [None]*len(df_header)\n",
    "    for i in range(len(df_header)):\n",
    "        word = df_header[i]\n",
    "        if word in review:\n",
    "            new_row[i] = review[word]\n",
    "    rows.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "narrow-singing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25812"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varying-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(df_header, [rows])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-consolidation",
   "metadata": {},
   "source": [
    "4) Convert the cumulative frequency count data content of your newly created Pandas dataframe into a \n",
    "NumPy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dutch-victoria",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b98bd9e7dcac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'set' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "word_set.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-vessel",
   "metadata": {},
   "source": [
    "5) Using the NumPy array, calculate the sparsity of the term-document matrix. What percentage of the \n",
    "entries in your term-document matrix contain zeroes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-lotus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-albuquerque",
   "metadata": {},
   "source": [
    "6) Next, using the content of the Pandas dataframe, plot the frequency distribution for the 30 words \n",
    "which occur most frequently in the positive reviews. What insights can you derive from the plot?7) Then, once again using the content of the Pandas dataframe, plot the frequency distribution for the 30 \n",
    "words which occur most frequently in the negative reviews. What insights can you derive from the \n",
    "plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-husband",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-labor",
   "metadata": {},
   "source": [
    "8) Now that you have successfully constructed and properly labeled the term-document matrix entries\n",
    "for each of the 2000 individual movie reviews, randomly sample 75% of the vectors contained within \n",
    "the term-document matrix for use as a model training data subset while leaving the remaining 25% of \n",
    "the vectors for the model testing data subset. How you choose to split the data is up to you as the data \n",
    "science / Python practitioner to decide. Be sure to display samples of your training and testing subsets \n",
    "to a reader of your work. Also, tell us how many documents are contained within your training subset? \n",
    "How many documents are contained in your testing subset? How many positive and negative reviews \n",
    "are contained within each subset? Does the mix of positive and negative reviews appear to be \n",
    "relatively balanced within each of the subsets? Be sure to provide a suitable explanatory narrative in \n",
    "the form of formatted Markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-pierce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "israeli-reserve",
   "metadata": {},
   "source": [
    "Your Jupyter notebook deliverable should contain (at a minimum) the following sections (including the relevant Python code for each \n",
    "section):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-dealer",
   "metadata": {},
   "source": [
    "1) Introduction (5 Points): Summarize the problem + explain the steps you plan to take to address the \n",
    "problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-worcester",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rental-parish",
   "metadata": {},
   "source": [
    "2) Data Preparation (40 Points): Describe + show the steps you have taken to load + transform the \n",
    "provided data into properly labeled count vectors within a Pandas-based Term-Document matrix. This \n",
    "section should include any Python code used for Data Preparation as well as an appropriate \n",
    "explanatory narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-devices",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "upper-bracelet",
   "metadata": {},
   "source": [
    "3) Calculate Matrix Sparsity (20 Points): Describe + show the steps you have taken to transform your \n",
    "Pandas dataframe to a NumPy array. Describe + show the steps you have taken to calculate the \n",
    "sparsity of your term-document matrix. This section should include any Python code used for Data \n",
    "Preparation as well as an appropriate explanatory narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-fusion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "grateful-chorus",
   "metadata": {},
   "source": [
    "4) Frequency Distribution Plots (20 Points): Explain + present your word count frequency distribution \n",
    "plots for the positive and negative reviews. This section should include any Python code used for \n",
    "creating the plots as well as an appropriate explanatory narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-advisory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "technological-ordinary",
   "metadata": {},
   "source": [
    "5) Sentiment Analysis Model Preparation (15 Points): Explain + present the process by which you \n",
    "separated the count vectors into training and testing subsets. This section should include any Python \n",
    "code used for creating the training + testing as well as an appropriate explanatory narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-biography",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "confused-walker",
   "metadata": {},
   "source": [
    "Your Jupyter Notebook deliverable should be similar to that of a publication-quality / professional caliber \n",
    "document and should include clearly labeled graphics, high-quality formatting, clearly defined section and \n",
    "sub-section headers, and be free of spelling and grammar errors. Furthermore, your Pythion code should \n",
    "include succinct explanatory comments. \n",
    "Upload / submit your Jupyter Notebook within the provided M12 Assignment Canvas submission portal. Be sure to \n",
    "save your Notebook using the following nomenclature: first initial_last name_M12_assn\" (e.g., J_Smith_M12_assn). Small groups should identity all group members at the start of the Jupyter Notebook \n",
    "and each team member should submit their own copy of the team’s work within Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-hunter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
