{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "military-stewart",
   "metadata": {},
   "source": [
    "Atreish Ramlakhan<br> Aishwarya Singh<br> Nosson Weissman<br>\n",
    "April 13, 2021 \n",
    "AIM 5001, Prof. James Topor  <br>\n",
    "\n",
    "# AIM 5001 Module 12 \n",
    "## Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-minority",
   "metadata": {},
   "source": [
    "As we’ve learned, many organizations rely on sentiment analysis algorithms to help them gauge the opinions \n",
    "of both existing and potential customers. For example, companies such as Amazon, TripAdvisor, Booking.com, \n",
    "WalMart, and Yelp (amongst others) apply sentiment analysis algorithms to the online product/service \n",
    "reviews provided by their customers to better understand how the public perceives competing products and \n",
    "services.\n",
    "Your task for the Module 12 Assignment is to prepare a collection of text documents for use within a \n",
    "sentiment analysis algorithm. The data set you will be working with is sourced from this site: \n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/. Specifically, you will be working with the polarity dataset \n",
    "v2.0, which is comprised of 1000 positive and 1000 negative movie reviews. Each movie review is in the form of free\u0002form text captured from web site postings. To complete this assignment you will need to make use of a fair \n",
    "amount of pre-processing techniques to prepare the content of the reviews for use within a classification \n",
    "model (e.g., strip out punctuation, stop words, “tokenize” the data, etc.).\n",
    "Get started on the Assignment as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "leading-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# from textblob import TextBlob\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "illegal-fraud",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'https://raw.githubusercontent.com/codepharmer/AIM-5001/main/Module_12/review_polarity.tar.gz'\n",
    "thetarfile = path\n",
    "ftpstream = urllib.request.urlopen(thetarfile)\n",
    "thetarfile = tarfile.open(fileobj=ftpstream, mode='r|gz')\n",
    "thetarfile.extractall('.')\n",
    "gz_dir = set([line.split('/')[0] for line in thetarfile.getnames()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-formula",
   "metadata": {},
   "source": [
    "1) Download the review_polarity.tar.gz file to your local environment and decompress its contents. The \n",
    "compressed file contains two directories: neg which contains 1000 negative movie reviews; and pos\n",
    "which contains 1000 positive movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-bottom",
   "metadata": {},
   "source": [
    "Your Jupyter notebook deliverable should contain (at a minimum) the following sections (including the relevant Python code for each \n",
    "section):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-hacker",
   "metadata": {},
   "source": [
    "**1) Introduction (5 Points):** Summarize the problem + explain the steps you plan to take to address the \n",
    "problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-lincoln",
   "metadata": {},
   "source": [
    "**Problem: Implement Sentiment Analysis For Data Set Consisting of Movie Reviews**\n",
    "- Import Data\n",
    "- Clean Data (tokenize, normalize, remove punctuation, stopwords, stem)\n",
    "- Create some sort of list for all reviews\n",
    "- Each entry in the list of reviews be a dict with word frequencies \n",
    "- Create a matrix with word frequencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-above",
   "metadata": {},
   "source": [
    "**2) Data Preparation (40 Points):** Describe + show the steps you have taken to load + transform the \n",
    "provided data into properly labeled count vectors within a Pandas-based Term-Document matrix. This \n",
    "section should include any Python code used for Data Preparation as well as an appropriate \n",
    "explanatory narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "independent-catholic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_textfiles_from_dir(path):\n",
    "    filenames = next(os.walk(path))[2]\n",
    "    files = []\n",
    "    for name in filenames:\n",
    "        files.append(open((path+'/'+name), 'r').read())\n",
    "    return files\n",
    "\n",
    "pos =  get_textfiles_from_dir('./txt_sentoken/pos')\n",
    "neg = get_textfiles_from_dir('./txt_sentoken/neg')\n",
    "# review_file = open('txt_sentoken')\n",
    "# [sent.lower() for sent in pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "packed-viking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "en_stops = stopwords.words('english')\n",
    "print (' ', ('not' in en_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-equality",
   "metadata": {},
   "source": [
    "**Some stop words in `en_stops` have the potential to change sentiment. An example of this is the word 'not'. For now we will not worry about this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "apparent-description",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.840803146362305\n",
      "17.71485209465027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19590"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "stemmer = nltk.porter.PorterStemmer()\n",
    "word_set = set()\n",
    "review_dicts = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "def process_string_list(string_list):\n",
    "    for i in range(len(string_list)):\n",
    "        word_freqs = dict()\n",
    "        review = string_list[i]\n",
    "        for word in tokenizer.tokenize(review):\n",
    "            text = stemmer.stem(word)\n",
    "#             if text not in string.punctuation and not text.isnumeric():\n",
    "            if text.isalpha()\\\n",
    "                and text not in en_stops:\n",
    "                    word_freqs[text] = word_freqs.get(text, 0) + 1\n",
    "                    word_set.add(text)\n",
    "        review_dicts.append(word_freqs)\n",
    "\n",
    "# start = time.time()    \n",
    "process_string_list(pos)\n",
    "# print(time.time() - start)\n",
    "# start = time.time()    \n",
    "process_string_list(pos)\n",
    "# print(time.time() - start)\n",
    "len(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-balance",
   "metadata": {},
   "source": [
    "**NOTE: Reviews are already down_cased as per the README.**\n",
    "\n",
    "_Within the folder \"txt_sentoken\" are the 2000 processed __down-cased__\n",
    "  text files used in Pang/Lee ACL 2004..._ <br>**As such, we did not need to use the .lower() method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "controlling-breakfast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-sullivan",
   "metadata": {},
   "source": [
    "3) Then, using a Jupyter Notebook, construct an algorithm (DO NOT USE scikit-learn COUNTVECTORIZER) \n",
    "that will read the content of each individual movie review from your new Github directories and \n",
    "convert that content into a properly labeled (i.e., POS / NEG or some appropriate proxy thereof) entry \n",
    "within a Pandas dataframe that encompasses all of the possible words contained within the 2000 \n",
    "movie reviews. When finished, the contents of your Pandas dataframe will constitute a term-document \n",
    "matrix for the movie review data. While constructing this term-document matrix within your Pandas \n",
    "dataframe, you should ensure that you remove any punctuation or stop words from the reviews. How \n",
    "you choose to manage the construction and proper labeling of the term-document matrix is up to you \n",
    "as the text mining / Python practitioner to decide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rocky-samba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_set\n",
    "df_header = list(word_set)\n",
    "df_header.sort()\n",
    "# review_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "discrete-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for review in review_dicts:\n",
    "    new_row = [0]*len(df_header)\n",
    "    for i in range(len(df_header)):\n",
    "        word = df_header[i]\n",
    "        if word in review:\n",
    "            new_row[i] = review[word]\n",
    "    rows.append(new_row)\n",
    "# first 1000 reviews in review_dict are postive. Second 100 reviews are negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "electrical-disease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25812"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "lined-prior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0009f</th>\n",
       "      <th>100m</th>\n",
       "      <th>10b</th>\n",
       "      <th>10th</th>\n",
       "      <th>11th</th>\n",
       "      <th>12th</th>\n",
       "      <th>13th</th>\n",
       "      <th>14th</th>\n",
       "      <th>150th</th>\n",
       "      <th>15th</th>\n",
       "      <th>...</th>\n",
       "      <th>zuko</th>\n",
       "      <th>zukovski</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zundel</th>\n",
       "      <th>zurg</th>\n",
       "      <th>zweibel</th>\n",
       "      <th>zwick</th>\n",
       "      <th>zwigoff</th>\n",
       "      <th>zyci</th>\n",
       "      <th>zzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 25812 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0009f  100m  10b  10th  11th  12th  13th  14th  150th  15th  ...  zuko  \\\n",
       "0         0     0    0     0     0     0     0     0      0     0  ...     0   \n",
       "1         0     0    0     0     0     0     0     0      0     0  ...     0   \n",
       "2         0     0    0     0     0     0     0     0      0     0  ...     0   \n",
       "3         0     0    0     0     0     0     0     0      0     0  ...     0   \n",
       "4         0     0    0     0     0     0     0     0      0     0  ...     0   \n",
       "...     ...   ...  ...   ...   ...   ...   ...   ...    ...   ...  ...   ...   \n",
       "1995      0     0    0     0     0     0     0     0      0     0  ...     0   \n",
       "1996      0     0    0     0     0     0     0     0      0     0  ...     0   \n",
       "1997      0     0    0     0     0     0     0     0      0     0  ...     0   \n",
       "1998      0     0    0     0     0     0     0     0      0     0  ...     0   \n",
       "1999      0     0    0     0     0     0     0     0      0     0  ...     0   \n",
       "\n",
       "      zukovski  zulu  zundel  zurg  zweibel  zwick  zwigoff  zyci  zzzzzzz  \n",
       "0            0     0       0     0        0      0        0     0        0  \n",
       "1            0     0       0     0        0      0        0     0        0  \n",
       "2            0     0       0     0        0      0        0     0        0  \n",
       "3            0     0       0     0        0      0        0     0        0  \n",
       "4            0     0       0     0        0      0        0     0        0  \n",
       "...        ...   ...     ...   ...      ...    ...      ...   ...      ...  \n",
       "1995         0     0       0     0        0      0        0     0        0  \n",
       "1996         0     0       0     0        0      0        0     0        0  \n",
       "1997         0     0       0     0        0      0        0     0        0  \n",
       "1998         0     0       0     0        0      0        0     0        0  \n",
       "1999         0     0       0     0        0      0        0     0        0  \n",
       "\n",
       "[2000 rows x 25812 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(rows, columns=df_header)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-cemetery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "present-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_reviews = CategorizedPlaintextCorpusReader(r'./txt_sentoken', r'.*\\.txt', cat_pattern=r'(\\w+)/*')\n",
    "# # print(movie_reviews.categories())\n",
    "# # print(movie_reviews.fileids())\n",
    "\n",
    "# documents = [(list(movie_reviews.words(fileid)), category)\n",
    "#              for category in ['pos', 'neg']\n",
    "#              for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "preceding-benchmark",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# movie_reviews.words(movie_reviews.fileids('pos')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-combining",
   "metadata": {},
   "source": [
    "2) Load the neg and pos directories to your AIM 5001 Github Repository. You need to keep the content of \n",
    "the directories separated since the directories themselves serve as the labels for the classification of \n",
    "the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accomplished-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_reviews = []\n",
    "# for fileid in movie_reviews.fileids('pos'):\n",
    "#     words = movie_reviews.words(fileid)\n",
    "#     pos_reviews.append(words)\n",
    "    \n",
    "# # print first 3 positive reviews from the pos_reviews list\n",
    "# print (pos_reviews[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-honolulu",
   "metadata": {},
   "source": [
    "**tokenize** <br>\n",
    "**text normalization** <br>\n",
    "**remove punctuation** <br>\n",
    "**removing emojis** <br>\n",
    "**removing stopwords** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-consolidation",
   "metadata": {},
   "source": [
    "4) Convert the cumulative frequency count data content of your newly created Pandas dataframe into a \n",
    "NumPy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dutch-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_count_arr = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "crude-platform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_count_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-vessel",
   "metadata": {},
   "source": [
    "5) Using the NumPy array, calculate the sparsity of the term-document matrix. What percentage of the \n",
    "entries in your term-document matrix contain zeroes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "governing-lotus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010010479621881296"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix is 98.99895% zeros\n"
     ]
    }
   ],
   "source": [
    "non_zero = np.count_nonzero(freq_count_arr)\n",
    "matrix_percent_nonzero =( non_zero/np.prod(freq_count_arr.shape))\n",
    "display(matrix_percent_nonzero)\n",
    "matrix_percent_zero = 1 - matrix_percent_nonzero\n",
    "matrix_percent_zero *= 100\n",
    "print('Matrix is {:0.5f}% zeros'.format(matrix_percent_zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "binding-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle \n",
    "shuffle(pos)\n",
    "shuffle(neg)\n",
    "\n",
    "# take 200 positive + 200 negative reviews to create a testing data set\n",
    "test_set = pos[:200] + neg[:200]\n",
    "# randomize the testing data set\n",
    "shuffle(test_set)\n",
    "\n",
    "# use the rest of the reviews for the training data set\n",
    "train_set = pos[200:] + neg[200:]\n",
    "# randomize the training data set\n",
    "shuffle(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dynamic-spank",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "numerous-affect",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-245f94e5a5f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train the Naive Bayes classifier using a training set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# now apply the naive bayes classifier to the test data set and check the accuracy of the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\classify\\naivebayes.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, labeled_featuresets, estimator)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;31m# Count up how many times each feature value occurred, given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;31m# the label and featurename.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m             \u001b[0mlabel_freqdist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# train the Naive Bayes classifier using a training set\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# now apply the naive bayes classifier to the test data set and check the accuracy of the result\n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-mambo",
   "metadata": {},
   "source": [
    "6) Next, using the content of the Pandas dataframe, plot the frequency distribution for the 30 words \n",
    "which occur most frequently in the positive reviews. What insights can you derive from the plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-albuquerque",
   "metadata": {},
   "source": [
    "7) Then, once again using the content of the Pandas dataframe, plot the frequency distribution for the 30 \n",
    "words which occur most frequently in the negative reviews. What insights can you derive from the \n",
    "plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-husband",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-labor",
   "metadata": {},
   "source": [
    "8) Now that you have successfully constructed and properly labeled the term-document matrix entries\n",
    "for each of the 2000 individual movie reviews, randomly sample 75% of the vectors contained within \n",
    "the term-document matrix for use as a model training data subset while leaving the remaining 25% of \n",
    "the vectors for the model testing data subset. How you choose to split the data is up to you as the data \n",
    "science / Python practitioner to decide. Be sure to display samples of your training and testing subsets \n",
    "to a reader of your work. Also, tell us how many documents are contained within your training subset? \n",
    "How many documents are contained in your testing subset? How many positive and negative reviews \n",
    "are contained within each subset? Does the mix of positive and negative reviews appear to be \n",
    "relatively balanced within each of the subsets? Be sure to provide a suitable explanatory narrative in \n",
    "the form of formatted Markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-pierce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "upper-bracelet",
   "metadata": {},
   "source": [
    "3) Calculate Matrix Sparsity (20 Points): Describe + show the steps you have taken to transform your \n",
    "Pandas dataframe to a NumPy array. Describe + show the steps you have taken to calculate the \n",
    "sparsity of your term-document matrix. This section should include any Python code used for Data \n",
    "Preparation as well as an appropriate explanatory narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-fusion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "grateful-chorus",
   "metadata": {},
   "source": [
    "4) Frequency Distribution Plots (20 Points): Explain + present your word count frequency distribution \n",
    "plots for the positive and negative reviews. This section should include any Python code used for \n",
    "creating the plots as well as an appropriate explanatory narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-advisory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "technological-ordinary",
   "metadata": {},
   "source": [
    "5) Sentiment Analysis Model Preparation (15 Points): Explain + present the process by which you \n",
    "separated the count vectors into training and testing subsets. This section should include any Python \n",
    "code used for creating the training + testing as well as an appropriate explanatory narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-biography",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "confused-walker",
   "metadata": {},
   "source": [
    "Your Jupyter Notebook deliverable should be similar to that of a publication-quality / professional caliber \n",
    "document and should include clearly labeled graphics, high-quality formatting, clearly defined section and \n",
    "sub-section headers, and be free of spelling and grammar errors. Furthermore, your Pythion code should \n",
    "include succinct explanatory comments. \n",
    "Upload / submit your Jupyter Notebook within the provided M12 Assignment Canvas submission portal. Be sure to \n",
    "save your Notebook using the following nomenclature: first initial_last name_M12_assn\" (e.g., J_Smith_M12_assn). Small groups should identity all group members at the start of the Jupyter Notebook \n",
    "and each team member should submit their own copy of the team’s work within Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-hunter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
